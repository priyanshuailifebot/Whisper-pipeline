# How to Verify Transcription is Working

## ğŸ” **Debugging Steps**

### **1. Check Browser Console**

After speaking, look for these messages in the browser console:

#### **Expected Messages:**
```
ğŸ“¨ Received message from server: {hasSegments: true, ...}
âœ… Received transcription segments: X segments
ğŸ“ Full transcription: [your spoken text]
ğŸ“¤ Calling onTranscript callback with: {...}
```

#### **If you see:**
- `ğŸ“¨ Received message from server` but no segments â†’ Server is sending messages but not transcriptions
- No messages at all â†’ WebSocket connection issue
- `âš ï¸ No segments in transcription message` â†’ Server sent empty transcription

### **2. Check WhisperLive Server Terminal**

Look for these log messages:

#### **Expected Messages:**
```
ğŸµ Audio processed chunk: 4096 samples, dtype: float32
ğŸ”Š Audio level: 0.XXXX (mean abs), max: 0.XXXX
ğŸ“¤ Sending transcription to client [uid]: X segments, text: '[your text]...'
âœ… Successfully sent X segments to client [uid]
```

#### **If you see:**
- `ğŸµ Audio processed chunk` â†’ Audio is being received and processed âœ…
- `ğŸ”Š Audio level` â†’ Audio has signal (should be > 0.01 for speech)
- `ğŸ“¤ Sending transcription` â†’ Transcription is being generated and sent âœ…
- No `ğŸ“¤ Sending transcription` â†’ Transcription not being generated (check below)

### **3. Verify Audio is Being Received**

#### **Check Audio Level:**
- If `audio_level` is very low (< 0.001), your microphone might not be picking up audio
- If `audio_level` is 0.0, no audio is being received
- Normal speech should be between 0.01 and 0.5

#### **Check VAD (Voice Activity Detection):**
- If VAD is enabled, it filters out non-speech audio
- Look for: `voice_active = True` in logs
- If `voice_active = False`, VAD is filtering out your audio

### **4. Common Issues and Solutions**

#### **Issue: No transcription messages in browser console**
**Possible causes:**
1. **Server not generating transcriptions**
   - Check server logs for `ğŸ“¤ Sending transcription`
   - If missing, check for errors in transcription process

2. **WebSocket connection issue**
   - Check browser console for WebSocket errors
   - Verify server is running on port 9090
   - Check network tab for WebSocket connection

3. **Audio not being sent**
   - Check browser console for: `ğŸ“‹ Sent client configuration to server`
   - Check for: `â–¶ï¸ Audio recording started`
   - Verify microphone permission is granted

#### **Issue: Audio processed but no transcription**
**Possible causes:**
1. **VAD filtering out audio**
   - Check if `voice_active = False` in server logs
   - Try speaking louder or closer to microphone
   - Check VAD threshold settings

2. **Model not loaded**
   - Check server startup logs for: `âœ… Model loaded`
   - If missing, model loading failed

3. **Transcription errors**
   - Check server logs for transcription errors
   - Look for `[ERROR]: Failed to transcribe audio chunk`

#### **Issue: Transcription appears in server logs but not in browser**
**Possible causes:**
1. **Message format mismatch**
   - Check browser console for: `âš ï¸ Unhandled message type`
   - Verify message has `segments` field

2. **onTranscript callback not working**
   - Check browser console for: `ğŸ“¤ Calling onTranscript callback`
   - Verify `handleVoiceInput` is being called in App.jsx

## ğŸ§ª **Quick Test Script**

Add this to your browser console to test the connection:

```javascript
// Check if WhisperLive client exists
if (window.whisperLiveClient) {
  console.log('âœ… WhisperLive client exists');
  console.log('Connection status:', window.whisperLiveClient.isConnected);
} else {
  console.log('âŒ WhisperLive client not found');
}

// Monitor WebSocket messages
const originalLog = console.log;
console.log = function(...args) {
  if (args[0]?.includes?.('transcription') || args[0]?.includes?.('segment')) {
    originalLog('ğŸ” TRANSCRIPTION DEBUG:', ...args);
  }
  originalLog.apply(console, args);
};
```

## ğŸ“Š **Expected Flow**

1. **Connection:**
   ```
   Browser â†’ WebSocket connect â†’ Server
   Server â†’ SERVER_READY â†’ Browser
   ```

2. **Audio Streaming:**
   ```
   Browser â†’ Audio chunks (float32) â†’ Server
   Server â†’ Process audio â†’ VAD â†’ Transcription
   ```

3. **Transcription:**
   ```
   Server â†’ {segments: [...]} â†’ Browser
   Browser â†’ handleTranscriptionMessage â†’ onTranscript â†’ handleVoiceInput â†’ UI
   ```

## âœ… **Verification Checklist**

- [ ] Server logs show: `ğŸµ Audio processed chunk`
- [ ] Server logs show: `ğŸ”Š Audio level: > 0.01`
- [ ] Server logs show: `ğŸ“¤ Sending transcription`
- [ ] Browser console shows: `ğŸ“¨ Received message from server`
- [ ] Browser console shows: `âœ… Received transcription segments`
- [ ] Browser console shows: `ğŸ“ Full transcription: [text]`
- [ ] Transcript appears in bottom control bar

## ğŸ› **If Still Not Working**

1. **Enable verbose logging:**
   - Server: Already enabled with DEBUG logs
   - Browser: Check console for all messages

2. **Test with a simple message:**
   - Speak clearly: "Hello, this is a test"
   - Wait 2-3 seconds
   - Check both server and browser logs

3. **Check network tab:**
   - Open browser DevTools â†’ Network tab
   - Filter by WS (WebSocket)
   - Check for messages being sent/received

4. **Verify microphone:**
   - Check system microphone settings
   - Test microphone in another app
   - Verify browser has microphone permission

---

**After adding the debug logging, restart both server and frontend, then speak and check the logs!**

